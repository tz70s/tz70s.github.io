<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<title> OpenWhisk Performance Improvement &middot; tz70s </title>


<link rel="stylesheet" href="https://tz70s.github.io/css/slim.css">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github-gist.min.css">
<link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Source+Code+Pro' rel='stylesheet'
  type='text/css'>


<link href="" rel="alternate" type="application/rss+xml" title="tz70s" />
</head>

<body>
    <div class="container">
        <div class="header">
    <div class="site-title"><a href="https://tz70s.github.io/">tz70s</a>  <a class="sub-site-title" href="https://github.com/tz70s">Github</a>   <a class="sub-site-title" href="https://twitter.com/tz70s">Twitter</a>          <a class="sub-site-title" href="https://linkedin.com/in/tzu-chiao-yeh-193697103">LinkedIn</a> 
    </div>
    <p class="site-tagline">About distributed system, programming and system level stuff.</p>
</div>
        <div class="content">
            <div class="posts">
                <div class="post">
                    <h2 class="post-title"><a href="https://tz70s.github.io/posts/openwhisk-performance-improvement/">OpenWhisk Performance Improvement</a></h2>
                    <span class="post-date">Jul 24, 2018 </span>
                    <div class="post-content">
                        <p>OpenWhisk community is nowadays getting more consistent on the new design of architecture for performance improvement. The <strong>future architecture</strong> of OpenWhisk requires large internal breaking changes. To fill the gap from idea into smooth migration, there might be helpful if a mid ground exist to clear more issues. Hence, I&rsquo;ve worked on prototyping performance improvement in real, but not that comprehensive though. However, hope this prototype hold a place for deeper discussion and discover all issues might meet in the future.</p>
<p>This post may have two kinds of target audiences, OpenWhisk community, GSoC mentors and reviewers:</p>
<h3 id="for-openwhisk-community">For OpenWhisk community:</h3>
<p>The objective of this post and protoype:</p>
<ul>
<li>Draft an experimental prototype of OpenWhisk future architecture, for knowing all issues might meet.</li>
<li>Hope to help community migrate to the futrue architecture smoothly.</li>
<li>Might be a starting point for further discussion.</li>
</ul>
<p>Something didn&rsquo;t implement/discuss in this post and experiment:</p>
<ul>
<li>Kubernetes: I only do prototyping for native (docker) side.</li>
<li>Logging.</li>
<li>Performance optimization on message queue.</li>
<li>Error path and tests.</li>
<li>Controller HA.</li>
<li>Scheduler persistence.</li>
</ul>
<h3 id="for-gsoc-mentors-rodric-and-carlos-and-reviewers">For GSoC mentors (Rodric and Carlos) and reviewers:</h3>
<p>This will be my final result during GSoC progress. But in GSoC guidance, please refer to <a href="https://tz70s.github.io/posts/gsoc-2018/">here for non-technical description</a>.</p>
<p>Full code can be found <a href="https://github.com/tz70s/incubator-openwhisk/tree/whisk-future-rebase">here</a>.</p>
<h1 id="architecture-recap">Architecture recap</h1>
<p>The <a href="https://cwiki.apache.org/confluence/display/OPENWHISK/OpenWhisk+future+architecture">Future Architecture proposal</a> proposed by Markus Thommes integrated lots of communities&rsquo; idea; play a great start to a new performant architecture.</p>
<h1 id="steps-to-produce">Steps to produce</h1>
<p>Instead of prototyping from clean slate, it&rsquo;s quite more challenged established via current codebase, but take more advantages:</p>
<ol>
<li>More accurate comparison on performance and architecture.</li>
<li>Help to explore the migration path.</li>
<li>Help me to better understanding on OpenWhisk internal implementation.</li>
</ol>
<p>However, OpenWhisk already provides nice infrastructure and utilities that I can easier to migrate this. As I mentioned, one of targets on this experiment can help to figure out the smooth migration path to future architecture. I&rsquo;ve tried my best not to break up current codebase and kept rebase it with upstream.</p>
<p>The prototyping steps as follow:</p>
<ol>
<li>A New LoadBalancer SPI: SingletonLoadBalancer.</li>
<li>Container Manager who manages container resources.</li>
<li>Re-use and refactor invoker-agent which is used by Kubernetes deployment.</li>
<li>In-detail implementation.</li>
</ol>
<p>I&rsquo;ll discuss these in detail below.</p>
<p>For cleaner naming convention, I&rsquo;ve renamed some words from proposal, here&rsquo;s the mapping:</p>
<ol>
<li>Container Management Agent -&gt; InvokerAgent</li>
<li>Container Manager -&gt; Scheduler</li>
</ol>
<h2 id="controller">Controller</h2>
<p>Most of Controller logic and programming semantics will not be changed, except for LoadBlancer. I&rsquo;ve created a new LoadBalancer implementation in <a href="https://github.com/tz70s/incubator-openwhisk/tree/whisk-future-rebase/core/controller/src/main/scala/whisk/core/loadBalancer/future">future package</a> and can be simply loaded with SPI infra. I&rsquo;ve not done the protocols with multiple Controllers, there&rsquo;s still some problems need to clarify that I&rsquo;ll point out below.</p>
<p>The overall LoadBalancer workflow as following:</p>
<ol>
<li>Call publish activation</li>
<li>Check if there&rsquo;s a warmed action existed, by looking up local container list.</li>
<li>If yes and if it&rsquo;s belong to owned, http call for it. (Else, redirect to another controller with carrying http client context.)</li>
<li>If not, queue in OverflowProxy and put the activation in the overflow queue.</li>
<li>Request for new resource, via proxy actor (sub actor in OverflowProxy actor) to Scheduler singleton.</li>
<li>After scheduler singleton&rsquo;s response arrived, update local container list (which generated via scheduler).</li>
<li>If it&rsquo;s belong to owned, http call for it. (Else, redirect to another controller with carrying http client context.)</li>
</ol>
<p>The code module contains:</p>
<h2 id="singleton-loadbalancer">Singleton LoadBalancer</h2>
<p>SingletonLoadBalancer implements LoadBalancer trait for SPI infra. Contrast to prior ShardingContainerLoadBalancer, we&rsquo;ll contain nothing about scheduling logic; it&rsquo;ll simply look up and pass requests. When a request come in, it&rsquo;ll lookup container lists, which contains some context related to container and in-flight concurrent requests. There will be some larger value once concurrent activation processing got finished, but current, the in-flight concurrent request can be only 0 or 1. That is, if a request reach 0 concurrency value, it reuse the existed free container and send request into ContainerProxy, or else, it&rsquo;ll send message to OverflowProxy for resource requisition.</p>
<!-- raw HTML omitted -->
<h2 id="overflow-proxy">Overflow Proxy</h2>
<p>Once OverflowProxy get message, it&rsquo;ll queue into Overflow Buffer. I didn&rsquo;t take external shared queue here, which many folks may concerned. But for Controller HA mode, it&rsquo;ll be required and can open up work-stealing capability. Anyway, the current implementation when receiving OverflowActivationMessage, queue in OverflowBuffer with some sendor and tracing context, and proxy to SingletonScheduler; and once it gets back with ContainerAllocation message, it&rsquo;ll pipe back to SingletonLoadBalancer.</p>
<h2 id="container-proxy">Container Proxy</h2>
<p>ContainerProxy is similar to prior invoke one, but I&rsquo;ll only manage with Suspend/Resume states and face to a warmed container. Therefore, the mission on ContainerProxy will operate suspend/resume (depends on pauseGrace settings) and call /run route of containers. Finally, pipe back result to SingletonLoadBalancer.</p>
<p>In order to make sure the correctness, I&rsquo;ve introduced two synchronized values here. It&rsquo;ll cause some problems while an <strong>in-flight</strong> activation is running but timer tick triggers and pause container. Therefore, it takes some paused here.</p>
<h2 id="sharestates-proxy">ShareStates Proxy</h2>
<p>Concerned some proposal didn&rsquo;t mention, the real Scheduling algorithm; We don&rsquo;t have any workload-dispatch related logics in Controller side, and make Scheduler holds all logics. However, what states Scheduler should know?</p>
<p>Consider building the prior busy/free/prewarmed pooling model:</p>
<ul>
<li>How do we know that which Container is busy and which is free?</li>
<li>How do we know that which Container is safe to delete and notify deletion?</li>
</ul>
<p>ShareStatesProxy do this: when Container gets pause/suspend, it&rsquo;ll notify Scheduler that container is busy or free. Once a deletion is being taken, it&rsquo;ll choose up from free pool (see more in Scheduler section) and notify back with ShareStatesProxy. Hence, ShareStatesProxy take an eval sharable container lists (actually a TrieMap) for updating and sharing with main SingletonLoadBalancer.</p>
<h2 id="container-factory-protocol">Container Factory Protocol</h2>
<p>Overall for interacting with Scheduler, the messages between Controller and Scheduler. You can see that the relations between Controller and Scheduler.</p>
<p>In convenient, I&rsquo;ve used Akka message passing, but this might not be ideal.</p>
<!-- raw HTML omitted -->
<h2 id="scheduler">Scheduler</h2>
<p>The Scheduler is responsible for handling controller resource requisitions, monitoring and (partially) mangaging containers&rsquo; lifecycle.
In the overall design, it&rsquo;s similar to most logic from prior Invoker, but without pause/resume operations.
I didn&rsquo;t deal this with Kubernetes. However, the approach may be similar and much simpler. Further, the new open source knative project might make the architecture further changed to adapt with Kubernetes.</p>
<p>To guarantee strong consistency for container selection in controller. The scheduler will be (contains) a cluster singleton to resolve container selection. <a href="https://doc.akka.io/docs/akka/2.5.14/cluster-singleton.html">Akka cluster singleton</a> is introduced here: ClusterSingletonManager will sits in Scheduler and ClusterSingletonProxy will sit in both Controller and Scheduler to access the singleton. You can checkout the doc for usage and <a href="https://github.com/tz70s/cluster-singleton-ex">here&rsquo;s my example with less noise</a> compared to Akka provided, which contains a basic singleton and proxy setup with migration observation guide. This enabled us to use with some convenience, i.e. failure recovery, message buffering, etc.</p>
<h2 id="container-pool-model">Container Pool Model</h2>
<p>Basically, I didn&rsquo;t change the model of container pool: busy/free/prewarm model. Simply review on this model with additional context introduce from new LoadBalancer design:</p>
<ol>
<li>Once the scheduler start, it&rsquo;ll create prewarm containers (via stem cell config).</li>
<li>A new request come in, it&rsquo;ll first match if there is a free and warmed container.</li>
<li>If yes, take it.</li>
<li>If not, match the prewarm with structural matching (kind, mem, etc.); If yes, take it.</li>
<li>If not, checkout there&rsquo;s enough slots, if yes, create one and take it.</li>
<li>If not, remove one container from free pool, create new one and take it.</li>
<li>Else, all containers are busy, we&rsquo;ll reject this request.</li>
</ol>
<p>The free and busy states are notified via ShareStatesProxy we have described above, caused by Container&rsquo;s suspend and resume.</p>
<!-- raw HTML omitted -->
<p>The algorithm is definitely not optimal, but we can first leave out previous problem: invokers have no visibility with each other.</p>
<h2 id="container-orchestration">Container Orchestration</h2>
<p>The Container Pool Model is based on single pool abstraction, to accomplish this model. We still need to have internal real resource allocation. The approach will be distincted by two orchestration choices: Kubernetes and Native (Docker).</p>
<ol>
<li>
<p><strong>Kubernetes</strong>:
Basically, we just call kubernetes api server with 1 to 1 mapping, it&rsquo;ll deal with all scheduling, health check and so on. Hence, in this case, the resource allocation is done by Kubernetes scheduling policy, with already well-defined resource matching and algorithm.</p>
</li>
<li>
<p><strong>Native(Docker)</strong>:
Contrast to indirectly calls in Kubernetes, in the native way, we&rsquo;ll have to deal everything by us. I&rsquo;ve introduced a simple and straight-forward algorithm: assign container to a node who has enough resources. This is definitely not ideal, i.e. node search is not scalable, easy fragmentation, etc. We can further improve this in the future.</p>
</li>
</ol>
<!-- raw HTML omitted -->
<p>You may find that there&rsquo;s an operation on port assignment. In order to do direct http calls, how can we reach warmed container after NAT environment?</p>
<p>In Kubernetes network model, the ip-per-pod model is flattened and we can easily work without this problem. But to the native mode, the warmed containers required some mechanisms to resolve with NAT environment.</p>
<p>There are some potential ways:</p>
<ol>
<li>
<p>Use InvokerAgent as a reverse proxy: this will take some additional benefits =&gt; reduce the calls after pause/resume, since Controller will no longer managed any Container&rsquo;s lifecycle. However, this is quite similar to prior Invoker, may gain some latency here.</p>
</li>
<li>
<p>Forced user to use somewhat overlay/underlay network policy, to reach a similar model in Kube.</p>
</li>
<li>
<p>Port-forwarding once creation.</p>
</li>
</ol>
<p>Obviously, it&rsquo;s arbitrary and not applicable for option 2. I choose 3 from now.</p>
<h2 id="new-container-trait">New Container Trait</h2>
<p>The responsibility on operating containers are segarated now:</p>
<ol>
<li>Pause/Resume on Controller.</li>
<li>Creation/Deletion on Scheduler.</li>
<li>Log on InvokerAgent.</li>
</ol>
<p>I&rsquo;ve splitted these into several different traits: <strong>ContainerRunnable</strong> for Controller side (ContainerProxy mixin it), <strong>ContainerLog</strong> for Invoker Agent, <strong>CoarseGrainContainer</strong> for Scheduler side and <strong>ContainerRestCall</strong> for sharing capability on doing rest calls.</p>
<p><a href="https://github.com/tz70s/incubator-openwhisk/blob/whisk-future-rebase/common/scala/src/main/scala/whisk/core/containerpool/future/Container.scala">You can find more detail here.</a></p>
<h2 id="issues-remain">Issues Remain</h2>
<ol>
<li>
<p>Perfromance bottleneck: Cluster singleton can easily reach performance bottleneck, therefore, we should try our best to reduce workload on it. First, the sophisticated scheduling should be scalable on large scale nodes: in Kubernetes, the scheduling is done by kube-scheduler (within API server). The better way is that Docker side should be also standalone as well, we can figure out later. Second, in which proposal didn&rsquo;t mention, to keep the free/prewarm/busy model the pause/unpause messages handling; they will be sent by every pause grace (500ms default) reach or new calls arrived.</p>
</li>
<li>
<p>Single point of failure: akka cluster singleton will automatically migrate to another scheduler node (actor-system) once the leader (oldest) getting down. This will result in short downtime, but it can be afford to us however, the ClusterSingletonProxy will buffer messages until the new singleton is up; the latency is overall make sense b.c. this is not actually the performance critical path.</p>
</li>
</ol>
<p>Some other issues I&rsquo;ll discuss below.</p>
<h2 id="invoker-agent">Invoker Agent</h2>
<p>Prior on Kubernetes deployment in OpenWhisk, it already has an invoker-agent do some similar jobs (pause, resume and log); and hence works fine for me with some basic refactoring.</p>
<p>However, this may not meet OpenWhisk requirements:</p>
<ol>
<li>
<p>One siginificant bottleneck and many folks concerned: <strong>activation log</strong>. We already had plenty of log store implementation, i.e. elasticsearch, docker file, etc. In general, we should not pass logs from whisk agent back to either Controller or Whisk Scheduler. Instead, directly store activation log into log store. Therefore, we can and we should reuse the codebase from implemented log store.</p>
</li>
<li>
<p>Better <strong>integration</strong>: Go has it&rsquo;s own workspace and directory convention ($GOPATH). It&rsquo;s neccessary if need a mature dependency management, i.e. dep. By convention, we might locate it into some kind of path: github.com/apache/incubator-openwhisk-invoker-agent, in other word, a separate repo; this definitely makes the project fractionized. Therefore, I vote for using scala-based implementation on InvokerAgent.</p>
</li>
</ol>
<p>Regardless which one to be reused, InvokerAgent is not the main target I&rsquo;m going to verify it. Use it is fined here, currently.</p>
<p>Basic functionalities of InvokerAgent:</p>
<ul>
<li><strong>Suspend</strong>: suspend/pause a specific container.</li>
</ul>
<p>GET  <a href="http://whisk.agent.host/suspend/container">http://whisk.agent.host/suspend/container</a></p>
<p>Will return status 204 on success and 500 on failure.</p>
<ul>
<li><strong>Resume</strong>: resume a specific container.</li>
</ul>
<p>GET  <a href="http://whisk.agent.host/resume/container">http://whisk.agent.host/resume/container</a></p>
<p>Will return status 204 on success and 500 on failure.</p>
<ul>
<li><strong>Log</strong>: read container logs and collect to a log sink file, and return back with JSON structure.</li>
</ul>
<p>GET <a href="http://whisk.agent.host/log/container">http://whisk.agent.host/log/container</a></p>
<p>Will return status 204 on success, 500 on general failure and 400 on log parsing failure.</p>
<p>That&rsquo;s all, there&rsquo;s only small changed from previous version; for more implementation detial, you can <a href="https://github.com/tz70s/incubator-openwhisk-deploy-kube/tree/refactor-invoker-agent/docker/invoker-agent">refer to here</a>.</p>
<h1 id="performance-tests">Performance tests</h1>
<p>It&rsquo;s sadly I don&rsquo;t have available resource to test the performance, simply use my macbook (2017, Core i5 2.3GHz, 8GB RAM, 128GB SSD) for benchmarking. For many reason (i.e. not sufficient resource, no runc optimization, resource interference, etc.), the performance is poor and not that accurate. Therefore, I can&rsquo;t give the arbitrary conclusion on performance improvement; but we can observer further issues during load tests.</p>
<h2 id="environment">Environment</h2>
<ul>
<li>MacBookPro 2017, Core i5 2.3GHz, 8GB RAM, 128GB SSD.</li>
<li>Docker for mac, version 18.06.0-ce-mac69, edge channel.</li>
<li>Tune docker machine to maximum value (4 vCPU, 8GB RAM)</li>
</ul>
<h2 id="setup">Setup</h2>
<p>For comparision, I&rsquo;ve setup:</p>
<ul>
<li>A full set of OpenWhisk (current architecture) with 1 Nginx, 1 Controller, 1 Kafka, 1 CouchDB and 2 Invokers.</li>
<li>New architecture of OpenWhisk with no Nginx (test via http), 1 Controller, 1 CouchDB, 1 Scheduler and 1 InvokerAgent (note that, the actual <strong>virtual node</strong> is two (2 container factories), but for local tests, I use 1 invokerAgent only).</li>
<li>Same limitation(no limit), same slots (numOfCores * coreShares), 50 millis pause grace, etc.</li>
<li>Benchmark via wrk, I&rsquo;ve extended the original wrk tests in OpenWhisk to <a href="https://github.com/tz70s/whisk-wrk-bench">multiple actions version</a>.</li>
<li>Running 1 action tests for 5 minutes and 4 action tests for 5 minutes.</li>
<li>wrk args: 10 * (number of actions) concurrency, 4 threads.</li>
<li>Benchmark are all start from 4 prewarmed containers.</li>
</ul>
<p>Note that I&rsquo;ve tried many times, the results are similar.</p>
<h2 id="current-architecture-1-action">Current architecture: 1 action</h2>
<p>As you can see, the resources are definitely overloaded. Note that it&rsquo;s not an accurate <strong>average</strong> latency under normal throttling.</p>
<pre><code class="language-bash">4 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   205.85ms  255.04ms   4.08s    85.18%
    Req/Sec    16.89     10.71    60.00     64.42%
  Latency Distribution
     50%   82.82ms
     75%  302.75ms
     90%  547.36ms
     99%  991.84ms
  15938 requests in 5.00m, 11.90MB read
Requests/sec:     53.11
Transfer/sec:     40.59KB
</code></pre>
<h2 id="current-architecture-4-actions">Current architecture: 4 actions</h2>
<p>The benchamrk result is definitely meaningless, it&rsquo;s heavily overloaded to run 4 actions. But the main reason leads to poor performance on 4 actions we already knows is the problem on no visiblity between Invokers. In addition, I&rsquo;ve found that there are easily causing failure starting from clean system, therefore, I&rsquo;ve post a pure start and a re-run result.</p>
<pre><code class="language-bash">4 threads and 40 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     5.87s     2.69s    9.59s    64.52%
    Req/Sec     0.83      1.99    10.00     89.67%
  Latency Distribution
     50%    6.32s
     75%    8.11s
     90%    8.89s
     99%    9.59s
  323 requests in 5.00m, 247.36KB read
  Socket errors: connect 88542, read 0, write 0, timeout 292
Requests/sec:      1.08
Transfer/sec:     844.07B
</code></pre>
<pre><code class="language-bash">4 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     1.38s     1.87s    9.79s    82.91%
    Req/Sec     6.73      5.87    48.00     82.41%
  Latency Distribution
     50%  390.06ms
     75%    2.11s
     90%    4.38s
     99%    7.45s
  3260 requests in 5.00m, 2.45MB read
  Socket errors: connect 0, read 0, write 0, timeout 2
Requests/sec:     10.86
Transfer/sec:      8.36KB
</code></pre>
<p>Video records for observation:</p>
<h2 id="new-architecture-1-actions">New architecture: 1 actions</h2>
<p>In the new architecture, for 1 action, I think it&rsquo;s arbitrary to say that it has greater performance, because of lacks of plenty functionalities (no logs collection, no ssl termination, etc). Please keep doubt to this estimated result. Note that you can see that there are some Non-2xx or 3xx responses, they are system rejection for overloaded, for early failure from scheduler, if all pools are in busy.</p>
<p>I&rsquo;ve also found pause/resume bug during high load, I&rsquo;ll described in the issue section at the bottom.</p>
<pre><code class="language-bash">4 threads and 10 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    11.37ms   50.32ms   1.02s    98.91%
    Req/Sec   131.36     48.86   232.00     64.19%
  Latency Distribution
     50%    6.60ms
     75%    8.63ms
     90%   10.77ms
     99%   66.42ms
  51199 requests in 5.00m, 28.66MB read
  Socket errors: connect 0, read 0, write 0, timeout 27
  Non-2xx or 3xx responses: 27
Requests/sec:    170.62
Transfer/sec:     97.80KB
</code></pre>
<h2 id="new-architecture-4-actions">New architecture: 4 actions</h2>
<p>As mentioned above, there is a pause/resume bug, therefore, the statistic is not accurate. Discuss below.</p>
<pre><code class="language-bash">4 threads and 40 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   207.64ms  962.41ms   9.98s    95.05%
    Req/Sec    78.16     97.98   390.00     79.54%
  Latency Distribution
     50%    4.28ms
     75%    7.53ms
     90%   43.68ms
     99%    5.62s
  8268 requests in 5.00m, 4.94MB read
  Socket errors: connect 0, read 0, write 0, timeout 185
  Non-2xx or 3xx responses: 3665
Requests/sec:     27.56
Transfer/sec:     16.86KB
</code></pre>
<h2 id="latency">Latency</h2>
<p>I&rsquo;ve used tracing support with Jaegar UI for troubleshooting and latency observation. Jaegar is compatible with Zipkin trace and painless deployment.</p>
<p>There are some average latency metrics in wrk above, mainly for non-pausing/resuming operations (direct calls), basically under 10ms.</p>
<p>Here&rsquo;s the tracing result for calls during warmed, but pause/resume. Latency gains for awaiting resuming.</p>
<p><img src="/images/latency.png" alt="Image"></p>
<p>In average, the latency on invocation averages in cross 10 seconds on my Mac; including container removal, create and initialization.</p>
<p><img src="/images/latency-cold-controller.png" alt="Image"></p>
<p>The bottleneck is initialization.</p>
<p><img src="/images/latency-cold-scheduler.png" alt="Image"></p>
<h1 id="conclusion--discussion">Conclusion &amp; Discussion</h1>
<p>There&rsquo;s still plenty of things not being done, and the approach is not ideal either; I&rsquo;ll keep work on this and hopes this help some OpenWhisk folks to join brain storming and find out the consistent design on future architecture of OpenWhisk.</p>
<h2 id="issues">Issues</h2>
<h3 id="its-neccessary-to-provide-a-strong-consistency-warmed-container-lists-to-be-kept-in-each-controller-how-do-we-deal-with-this">It&rsquo;s neccessary to provide a strong consistency (warmed) container lists to be kept in each controller, how do we deal with this?</h3>
<p>Using cluster singleton in Scheduler is mainly for providing strong consistency on managing containers&rsquo; states. However, lookup states store in Scheduler will lead to poor performance (gains latency and makes singleton busy).</p>
<p>To release the burden and latency on lookup Scheduler, we can keep only owned container lists (actually a map) in each in-memory store (as I did and proposal mentioned that). However, introduce cache will cause temporary inconsistent. How do we solve this?</p>
<h3 id="how-do-we-deal-with-overflow-messages">How do we deal with overflow messages?</h3>
<p>There&rsquo;s not consistent here due to issues related to message queues (i.e. unbounded topics, using pub/sub, etc.). I&rsquo;ll experiment these further as well.</p>
<h3 id="problematics-on-containerproxy">Problematics on ContainerProxy</h3>
<p>In order to guarantee correctness, I&rsquo;ve used blocking Await in ContainerProxy. There&rsquo;s a potential problem is: when an activation is running, we can&rsquo;t call pause on it. I&rsquo;ll keep figure out this problem as soon as possible.</p>
<h3 id="states-for-freebusy-pool-recognition">States for free/busy pool recognition?</h3>
<p>Is the state related to free/busy == pause/resume?</p>
<p>There&rsquo;s a potential tradeoff is: if pause/resume becoming bottleneck, we would like to use greater pause grace. However, greater pause grace means that the container pool is easier to be full of busy states. Ideally, in runc optimization, there&rsquo;s no problem at all. However, in docker version pause/suspend, this may need to be concerned.</p>
<h3 id="peek-container-requisition">Peek container requisition.</h3>
<p>When burst requests are coming (plenty of cold start occurred), the Scheduler will response to over-estimated resources to system. We should further introduce scheduling algorithm improvement. However, it&rsquo;s great that we have a single place (Scheduler) to consider and experiment overall resource allocation.</p>
<h3 id="will-it-be-performance-bottleneck-on-scheduler-singleton">Will it be performance bottleneck on Scheduler Singleton?</h3>
<p>Actually, Scheduler can not only keep create and deletion states; as more states come in, will it be a performance bottleneck on large scale system?</p>

                    </div>
                </div>
                <div class="pagination">
                    <a class="btn previous " href="https://tz70s.github.io/posts/dist-collections/"> Prev</a>  
                    <a class="btn next " href="https://tz70s.github.io/posts/gsoc-2018/"> Next</a> 
                </div>
            </div>
        </div>
         <div class="footer">
  
  <p>Tzu-Chiao Yeh { @tz70s, su3g4284zo6y7@gmail.com }</p>
  
</div>

    </div>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/haskell.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/scala.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/languages/fsharp.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    
</body>

</html>